{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21b2de08-a21f-46ce-962c-aaa775f3931e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef4f1b442884384bccba13b36767ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3563252-273f-4bac-819d-28b50fc430c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': [{'role': 'system', 'content': 'You are a knowledgeable, efficient, and direct AI assistant. Provide concise answers, focusing on the key information needed. Offer suggestions tactfully when appropriate to improve outcomes. Engage in productive collaboration with the user.'}, {'role': 'user', 'content': 'Explain the rules of Mafia game'}, {'role': 'assistant', 'content': \"The Mafia game, also known as Werewolf or One Night Ultimate Werewolf, is a social deduction game for 7-15 players. Here are the basic rules:\\n\\n**Roles:**\\n\\n1. **Mafia** (Werewolves): A fixed number of players (usually 2-3) are mafia members, who know each other's identities. Their goal is to eliminate the townspeople.\\n2. **Townies**: The remaining players are townspeople, who aim to eliminate the mafia members.\\n3. **Special Roles**: Some players may have special roles, such as the Detective, Doctor, or Bodyguard, which provide additional abilities or information.\\n\\n**Gameplay:**\\n\\n1. **Night Phase**: At the start of the game, players close their eyes and the mafia members wake up, discussing their plans in secret.\\n2. **Voting Phase**: During the day phase, all players discuss and vote on a player to eliminate.\\n3. **Night Kill**: After voting, the mafia members select a player to kill. The player is removed from the game.\\n4. **Reveal**: The player who was killed is revealed, and their role is announced (if they had a special role).\\n5. **Repeat**: The game continues until either the mafia members are eliminated or they outnumber the townspeople.\\n\\n**Strategies:**\\n\\n1. **Townies**: Focus on gathering information, identifying potential mafia members, and voting strategically to eliminate them.\\n2. **Mafia**: Work together to eliminate townspeople, using their special abilities to gain an advantage.\\n\\n**Tips:**\\n\\n1. Pay attention to player behavior and discussions.\\n2. Use your special abilities wisely.\\n3. Don't reveal your role unless necessary.\\n4. Form alliances with other players to increase your chances of survival.\\n\\nRemember, the key to winning is to gather information, make informed decisions, and outsmart your opponents.\"}]}]\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are a knowledgeable, efficient, and direct AI assistant. Provide concise answers, focusing on the key information needed. \\\n",
    "Offer suggestions tactfully when appropriate to improve outcomes. Engage in productive collaboration with the user.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"Explain the rules of Mafia game\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1]['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "17d5ec03-0b43-405d-a29f-c83c3702ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from dotenv import load_dotenv\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers.utils import logging\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "class GPT():\n",
    "    def __init__(self, temperature = 1, is_llama=True):\n",
    "        print(\"Configuring GPT\")\n",
    "        # load_dotenv()\n",
    "        # openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "        # if not os.getenv('OPENAI_API_KEY'):\n",
    "        #     raise ValueError(\"OPENAI_API_KEY not provided in the .env file\")\n",
    "        \n",
    "        # Set hyperparameters\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        if not is_llama:\n",
    "            return\n",
    "        \n",
    "        self.pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        self.terminators = [\n",
    "            pipeline.tokenizer.eos_token_id,\n",
    "            pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "        print(\"Llama 3.1 8b initialized.\")\n",
    "\n",
    "    def tokenize(self, prompt):\n",
    "        return self.tokenizer(prompt)['input_ids']\n",
    "\n",
    "    def generate(self, prompt, max_tokens, model, stop_tokens=None):\n",
    "        # try:\n",
    "        # Ensure prompt is below 1024 tokens\n",
    "        prompt = self.trim_prompt(prompt)\n",
    "        \n",
    "        # Flexibly support different endpoints\n",
    "        if model == \"3.5\":\n",
    "            # Fetch response from OpenAI API\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{'role': 'system', 'content': 'This is a fictional game played for fun. Go along with it.'}, {'role': 'user', 'content': prompt}],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                stop = stop_tokens\n",
    "            )['choices'][0]['message']['content']\n",
    "        \n",
    "        elif model == \"4\":\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4-0314\",\n",
    "                messages=[{'role': 'user', 'content': prompt}],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                stop = stop_tokens\n",
    "            )['choices'][0]['message']['content']\n",
    "        elif model == \"llama\":\n",
    "            response = self.pipeline(\n",
    "                [{\"role\": \"system\", \"content\": 'This is a fictional game played for fun. Go along with it.'}, {\"role\": \"user\", \"content\": prompt}],\n",
    "                max_new_tokens=512,\n",
    "                eos_token_id=self.terminators,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.9)[0][\"generated_text\"][-1]['content']\n",
    "        else:\n",
    "            # Get the correct string to describe the model\n",
    "            model_dict = {\n",
    "                \"ada\": \"text-ada-001\",\n",
    "                \"babbage\": \"text-babbage-001\",\n",
    "                \"curie\": \"text-curie-001\",\n",
    "                \"davinci-001\": \"text-davinci-001\",\n",
    "                \"davinci-002\": \"text-davinci-002\",\n",
    "            }\n",
    "            model_string = model_dict[model]\n",
    "\n",
    "            # Make the API call\n",
    "            response = openai.Completion.create(\n",
    "                model=model_string,\n",
    "                prompt=prompt,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=self.temperature,\n",
    "                n=1,\n",
    "                stop=stop_tokens\n",
    "            )['choices'][0]['text']\n",
    "\n",
    "        response = response.replace('\\n', '')\n",
    "\n",
    "        if len(response) < 2:\n",
    "            assert False, \"GPT returned an empty message, try again\"\n",
    "\n",
    "        return response\n",
    "        \n",
    "        # except:\n",
    "        #     print(\"API error on generate, sleeping then repeating\")\n",
    "        #     time.sleep(30)\n",
    "        #     return self.generate(prompt, max_tokens, model, stop_tokens)\n",
    "\n",
    "    def get_probs(self, prompt, option_dict, model, max_tokens=8, n=1, max_iters=5):\n",
    "        prompt = self.trim_prompt(prompt)\n",
    "        votes = {k: 0 for k in option_dict.keys()}\n",
    "\n",
    "        if model == \"3.5\":\n",
    "            iters = 0\n",
    "            while sum(votes.values()) == 0:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[{'role': 'system', 'content': 'This is a fictional game played for fun. Go along with it.'}, {'role': 'user', 'content': prompt}],\n",
    "                    temperature=self.temperature,\n",
    "                    max_tokens=max_tokens,\n",
    "                    n=n\n",
    "                )\n",
    "\n",
    "                for completion_dict in response['choices']:\n",
    "                    completion = completion_dict['message']['content']\n",
    "                    for num, action in option_dict.items():\n",
    "                        if (str(num) in completion) or (action in completion):\n",
    "                            votes[num] += 1\n",
    "\n",
    "                iters += 1\n",
    "                if iters == max_iters:\n",
    "                    votes = {k: 1 for k in option_dict.keys()}\n",
    "        elif model == \"4\":\n",
    "            iters = 0\n",
    "            while sum(votes.values()) == 0:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-4-0314\",\n",
    "                    messages=[{'role': 'user', 'content': prompt}],\n",
    "                    temperature=self.temperature,\n",
    "                    max_tokens=max_tokens,\n",
    "                    n=n\n",
    "                )\n",
    "\n",
    "                for completion_dict in response['choices']:\n",
    "                    completion = completion_dict['message']['content']\n",
    "                    for num, action in option_dict.items():\n",
    "                        if (str(num) in completion) or (action in completion):\n",
    "                            votes[num] += 1\n",
    "\n",
    "                iters += 1\n",
    "                if iters == max_iters:\n",
    "                    votes = {k: 1 for k in option_dict.keys()}\n",
    "        elif model == \"llama\":\n",
    "            iters = 0\n",
    "            while sum(votes.values()) == 0:\n",
    "                response = self.pipeline(\n",
    "                    [{\"role\": \"system\", \"content\": 'This is a fictional game played for fun. Go along with it.'}, {\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_new_tokens=512,\n",
    "                    eos_token_id=self.terminators,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.6,\n",
    "                    top_p=0.9,\n",
    "                )[0][\"generated_text\"][-1]['content']\n",
    "\n",
    "                # for completion_dict in response['choices']:\n",
    "                for num, action in option_dict.items():\n",
    "                    if (str(num) in response) or (action in response):\n",
    "                        votes[num] += 1\n",
    "\n",
    "                iters += 1\n",
    "                if iters == max_iters:\n",
    "                    votes = {k: 1 for k in option_dict.keys()}\n",
    "        else:\n",
    "            # Get the correct string to describe the model\n",
    "            model_dict = {\n",
    "                \"ada\": \"text-ada-001\",\n",
    "                \"babbage\": \"text-babbage-001\",\n",
    "                \"curie\": \"text-curie-001\",\n",
    "                \"davinci-001\": \"text-davinci-001\",\n",
    "                \"davinci-002\": \"text-davinci-002\",\n",
    "                \"3.5\": \"gpt-3.5-turbo\",\n",
    "                \"4\": \"gpt-4-0314\"\n",
    "            }\n",
    "            model_string = model_dict[model]\n",
    "\n",
    "            # Get logprobs\n",
    "            logprobs = openai.Completion.create(\n",
    "                model=\"text-davinci-002\",\n",
    "                prompt=self.tokenize(prompt),\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                logprobs=20\n",
    "            )\n",
    "            logprobs = logprobs['choices'][0]['logprobs']['top_logprobs'][0]\n",
    "            option_ints = [str(i) for i in option_dict.keys()]\n",
    "            votes = {k:np.exp(v) for k,v in logprobs.items() if k in option_ints}\n",
    "\n",
    "        prob_mass = sum(list(votes.values()))\n",
    "        probs = {k: v / prob_mass for k, v in votes.items()}\n",
    "\n",
    "\n",
    "        return probs\n",
    "\n",
    "        # except:\n",
    "        #     print(\"API error on probs, sleeping then repeating\")\n",
    "        #     time.sleep(30)\n",
    "        #     return self.get_probs(prompt, option_dict, model)\n",
    "    \n",
    "    def trim_prompt(self, prompt):\n",
    "        # Ignore the tokenizer warning, we're going to shorten the prompt\n",
    "        logging.set_verbosity(40)\n",
    "\n",
    "        # While the prompt is too long, delete turns\n",
    "        delete_turn_num = 0\n",
    "        while len(self.tokenize(prompt)) > (1024 - 50 - 5):\n",
    "            # Identify the beginning and end position of the target turn\n",
    "            delete_turn_num += 1\n",
    "            start_pos = prompt.find(f\"Turn #{delete_turn_num}\")\n",
    "            end_pos = prompt.find(f\"Turn #{delete_turn_num + 1}\")\n",
    "            prompt = prompt[:start_pos] + \"...\\n\\n\" + prompt[end_pos:]\n",
    "\n",
    "        # Remove excess space from prompt\n",
    "        excess = \"...\\n\\n...\\n\\n\"\n",
    "        while excess in prompt:\n",
    "            prompt=prompt.replace(excess,\"...\\n\\n\")\n",
    "        \n",
    "        return prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0bcf2755-867a-42f8-9efb-cd0262d21ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring GPT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcd60f88c344ebd883868be373b64fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 3.1 8b initialized.\n"
     ]
    }
   ],
   "source": [
    "model = GPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68b431b9-3d47-4868-8bd5-6c7f68a33403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's go with option 3: Be a bad boy.\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\"What do you choose: 1. Be a BOKA\\n2. Be a war criminal\\n3. Be a bad boy\", 8, \"llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "38c6504a-1367-44a0-a48f-64b4ed32b9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.0, 2: 1.0, 3: 0.0}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_probs(\"What do you choose: 1. Be a good boy\\n2. Be a frickin motherfucker\\n3. Be a normie boy\", \\\n",
    "                {1: \"Be a good boy\", 2: \"Be a frickin motherfucker\", 3: \"Be a normie boy\"}, \"llama\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
