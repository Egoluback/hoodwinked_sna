{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21b2de08-a21f-46ce-962c-aaa775f3931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3563252-273f-4bac-819d-28b50fc430c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'terminators' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are a knowledgeable, efficient, and direct AI assistant. Provide concise answers, focusing on the key information needed. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mOffer suggestions tactfully when appropriate to improve outcomes. Engage in productive collaboration with the user.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt},\n\u001b[1;32m      6\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain the rules of Mafia game\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      7\u001b[0m ]\n\u001b[1;32m      9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mpipeline(\n\u001b[1;32m     10\u001b[0m     messages,\n\u001b[1;32m     11\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[0;32m---> 12\u001b[0m     eos_token_id\u001b[38;5;241m=\u001b[39m\u001b[43mterminators\u001b[49m,\n\u001b[1;32m     13\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m,\n\u001b[1;32m     15\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'terminators' is not defined"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are a knowledgeable, efficient, and direct AI assistant. Provide concise answers, focusing on the key information needed. \\\n",
    "Offer suggestions tactfully when appropriate to improve outcomes. Engage in productive collaboration with the user.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"Explain the rules of Mafia game\"},\n",
    "]\n",
    "\n",
    "outputs = transformers.pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1]['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d5ec03-0b43-405d-a29f-c83c3702ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from dotenv import load_dotenv\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers.utils import logging\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "class GPT():\n",
    "    def __init__(self, temperature = 1, is_llama=True):\n",
    "        print(\"Configuring GPT\")\n",
    "        # load_dotenv()\n",
    "        # openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "        # if not os.getenv('OPENAI_API_KEY'):\n",
    "        #     raise ValueError(\"OPENAI_API_KEY not provided in the .env file\")\n",
    "        \n",
    "        # Set hyperparameters\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        if not is_llama:\n",
    "            return\n",
    "        \n",
    "        self.pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        self.terminators = [\n",
    "            pipeline.tokenizer.eos_token_id,\n",
    "            pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "        print(\"Llama 3.1 8b initialized.\")\n",
    "\n",
    "    def tokenize(self, prompt):\n",
    "        return self.tokenizer(prompt)['input_ids']\n",
    "\n",
    "    def generate(self, prompt, max_tokens, model, stop_tokens=None):\n",
    "        # try:\n",
    "        # Ensure prompt is below 1024 tokens\n",
    "        prompt = self.trim_prompt(prompt)\n",
    "        \n",
    "        # Flexibly support different endpoints\n",
    "        if model == \"3.5\":\n",
    "            # Fetch response from OpenAI API\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{'role': 'system', 'content': 'This is a fictional game played for fun. Go along with it.'}, {'role': 'user', 'content': prompt}],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                stop = stop_tokens\n",
    "            )['choices'][0]['message']['content']\n",
    "        \n",
    "        elif model == \"4\":\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4-0314\",\n",
    "                messages=[{'role': 'user', 'content': prompt}],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                stop = stop_tokens\n",
    "            )['choices'][0]['message']['content']\n",
    "        elif model == \"llama\":\n",
    "            response = self.pipeline(\n",
    "                [{\"role\": \"system\", \"content\": 'This is a fictional game played for fun. Go along with it.'}, {\"role\": \"user\", \"content\": prompt}],\n",
    "                max_new_tokens=512,\n",
    "                eos_token_id=self.terminators,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,\n",
    "                top_p=0.9)[0][\"generated_text\"][-1]['content']\n",
    "        else:\n",
    "            # Get the correct string to describe the model\n",
    "            model_dict = {\n",
    "                \"ada\": \"text-ada-001\",\n",
    "                \"babbage\": \"text-babbage-001\",\n",
    "                \"curie\": \"text-curie-001\",\n",
    "                \"davinci-001\": \"text-davinci-001\",\n",
    "                \"davinci-002\": \"text-davinci-002\",\n",
    "            }\n",
    "            model_string = model_dict[model]\n",
    "\n",
    "            # Make the API call\n",
    "            response = openai.Completion.create(\n",
    "                model=model_string,\n",
    "                prompt=prompt,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=self.temperature,\n",
    "                n=1,\n",
    "                stop=stop_tokens\n",
    "            )['choices'][0]['text']\n",
    "\n",
    "        response = response.replace('\\n', '')\n",
    "\n",
    "        if len(response) < 2:\n",
    "            assert False, \"GPT returned an empty message, try again\"\n",
    "\n",
    "        return response\n",
    "        \n",
    "        # except:\n",
    "        #     print(\"API error on generate, sleeping then repeating\")\n",
    "        #     time.sleep(30)\n",
    "        #     return self.generate(prompt, max_tokens, model, stop_tokens)\n",
    "\n",
    "    def get_probs(self, prompt, option_dict, model, max_tokens=8, n=1, max_iters=5):\n",
    "        prompt = self.trim_prompt(prompt)\n",
    "        votes = {k: 0 for k in option_dict.keys()}\n",
    "\n",
    "        if model == \"3.5\":\n",
    "            iters = 0\n",
    "            while sum(votes.values()) == 0:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[{'role': 'system', 'content': 'This is a fictional game played for fun. Go along with it.'}, {'role': 'user', 'content': prompt}],\n",
    "                    temperature=self.temperature,\n",
    "                    max_tokens=max_tokens,\n",
    "                    n=n\n",
    "                )\n",
    "\n",
    "                for completion_dict in response['choices']:\n",
    "                    completion = completion_dict['message']['content']\n",
    "                    for num, action in option_dict.items():\n",
    "                        if (str(num) in completion) or (action in completion):\n",
    "                            votes[num] += 1\n",
    "\n",
    "                iters += 1\n",
    "                if iters == max_iters:\n",
    "                    votes = {k: 1 for k in option_dict.keys()}\n",
    "        elif model == \"4\":\n",
    "            iters = 0\n",
    "            while sum(votes.values()) == 0:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-4-0314\",\n",
    "                    messages=[{'role': 'user', 'content': prompt}],\n",
    "                    temperature=self.temperature,\n",
    "                    max_tokens=max_tokens,\n",
    "                    n=n\n",
    "                )\n",
    "\n",
    "                for completion_dict in response['choices']:\n",
    "                    completion = completion_dict['message']['content']\n",
    "                    for num, action in option_dict.items():\n",
    "                        if (str(num) in completion) or (action in completion):\n",
    "                            votes[num] += 1\n",
    "\n",
    "                iters += 1\n",
    "                if iters == max_iters:\n",
    "                    votes = {k: 1 for k in option_dict.keys()}\n",
    "        elif model == \"llama\":\n",
    "            iters = 0\n",
    "            while sum(votes.values()) == 0:\n",
    "                response = self.pipeline(\n",
    "                    [{\"role\": \"system\", \"content\": 'This is a fictional game played for fun. Go along with it.'}, {\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_new_tokens=512,\n",
    "                    eos_token_id=self.terminators,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.6,\n",
    "                    top_p=0.9,\n",
    "                )[0][\"generated_text\"][-1]['content']\n",
    "\n",
    "                # for completion_dict in response['choices']:\n",
    "                for num, action in option_dict.items():\n",
    "                    if (str(num) in response) or (action in response):\n",
    "                        votes[num] += 1\n",
    "\n",
    "                iters += 1\n",
    "                if iters == max_iters:\n",
    "                    votes = {k: 1 for k in option_dict.keys()}\n",
    "        else:\n",
    "            # Get the correct string to describe the model\n",
    "            model_dict = {\n",
    "                \"ada\": \"text-ada-001\",\n",
    "                \"babbage\": \"text-babbage-001\",\n",
    "                \"curie\": \"text-curie-001\",\n",
    "                \"davinci-001\": \"text-davinci-001\",\n",
    "                \"davinci-002\": \"text-davinci-002\",\n",
    "                \"3.5\": \"gpt-3.5-turbo\",\n",
    "                \"4\": \"gpt-4-0314\"\n",
    "            }\n",
    "            model_string = model_dict[model]\n",
    "\n",
    "            # Get logprobs\n",
    "            logprobs = openai.Completion.create(\n",
    "                model=\"text-davinci-002\",\n",
    "                prompt=self.tokenize(prompt),\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                logprobs=20\n",
    "            )\n",
    "            logprobs = logprobs['choices'][0]['logprobs']['top_logprobs'][0]\n",
    "            option_ints = [str(i) for i in option_dict.keys()]\n",
    "            votes = {k:np.exp(v) for k,v in logprobs.items() if k in option_ints}\n",
    "\n",
    "        prob_mass = sum(list(votes.values()))\n",
    "        probs = {k: v / prob_mass for k, v in votes.items()}\n",
    "\n",
    "\n",
    "        return probs\n",
    "\n",
    "        # except:\n",
    "        #     print(\"API error on probs, sleeping then repeating\")\n",
    "        #     time.sleep(30)\n",
    "        #     return self.get_probs(prompt, option_dict, model)\n",
    "    \n",
    "    def trim_prompt(self, prompt):\n",
    "        # Ignore the tokenizer warning, we're going to shorten the prompt\n",
    "        logging.set_verbosity(40)\n",
    "\n",
    "        # While the prompt is too long, delete turns\n",
    "        delete_turn_num = 0\n",
    "        while len(self.tokenize(prompt)) > (1024 - 50 - 5):\n",
    "            # Identify the beginning and end position of the target turn\n",
    "            delete_turn_num += 1\n",
    "            start_pos = prompt.find(f\"Turn #{delete_turn_num}\")\n",
    "            end_pos = prompt.find(f\"Turn #{delete_turn_num + 1}\")\n",
    "            prompt = prompt[:start_pos] + \"...\\n\\n\" + prompt[end_pos:]\n",
    "\n",
    "        # Remove excess space from prompt\n",
    "        excess = \"...\\n\\n...\\n\\n\"\n",
    "        while excess in prompt:\n",
    "            prompt=prompt.replace(excess,\"...\\n\\n\")\n",
    "        \n",
    "        return prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bcf2755-867a-42f8-9efb-cd0262d21ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring GPT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/uvd174/miniconda3/envs/hoodwinked_sna/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936462a0436145b8972b3e4fc25624af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/uvd174/miniconda3/envs/hoodwinked_sna/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d62cce8fcdf4dc9bc9410aa76434e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model meta-llama/Llama-3.1-8B-Instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m, in \u001b[0;36mGPT.__init__\u001b[0;34m(self, temperature, is_llama)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_llama:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-3.1-8B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch_dtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminators \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     36\u001b[0m     pipeline\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m     37\u001b[0m     pipeline\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m ]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama 3.1 8b initialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/storage/uvd174/miniconda3/envs/hoodwinked_sna/lib/python3.9/site-packages/transformers/pipelines/__init__.py:774\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;66;03m# Infer the framework from the model\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;66;03m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;66;03m# Will load the correct model if possible\u001b[39;00m\n\u001b[1;32m    773\u001b[0m model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 774\u001b[0m framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    784\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    785\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m/storage/uvd174/miniconda3/envs/hoodwinked_sna/lib/python3.9/site-packages/transformers/pipelines/base.py:271\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    273\u001b[0m framework \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.engine.training.Model\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(inspect\u001b[38;5;241m.\u001b[39mgetmro(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m framework, model\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model meta-llama/Llama-3.1-8B-Instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>)."
     ]
    }
   ],
   "source": [
    "model = GPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68b431b9-3d47-4868-8bd5-6c7f68a33403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's go with option 3: Be a bad boy.\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\"What do you choose: 1. Be a BOKA\\n2. Be a war criminal\\n3. Be a bad boy\", 8, \"llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "38c6504a-1367-44a0-a48f-64b4ed32b9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.0, 2: 1.0, 3: 0.0}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_probs(\"What do you choose: 1. Be a good boy\\n2. Be a frickin motherfucker\\n3. Be a normie boy\", \\\n",
    "                {1: \"Be a good boy\", 2: \"Be a frickin motherfucker\", 3: \"Be a normie boy\"}, \"llama\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
